We are going to recast the estimation problem from the previous section in the framework of the Kalman filter, which is based in the following two equations ({\em state evolution} and {\em measurement}):
\begin{equation}\label{eq:statespace}
  \bm s_n = \bm A_n \bm s_{n-1} + \bm G_n \bm f_n + \bm u_n, \qquad \bm x_n = \bm H_n \bm s_n + \bm w_n.
\end{equation}
(The definitions of the different quantities should be familiar, and can be found in the class notes).
In our problem we have a single variable to estimate (the fluid level), so the state vector $\bm s_n$ is actually a scalar $s_n$. At time $n$ we obtain a single new pressure measurement, so the vector $\bm x_n$ also reduces to a scalar, $x_n$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
\question{Question: Find a ``state evolution'' and ``measurement'' description for \eqref{eq:simple_obs} in the form of \eqref{eq:statespace}.
  Note that the state of the system (the fluid level) is static, i.e., $s_n = s$ is constant with $n$.}
\vspace{0.5cm}

Given that the fluid level is static ($s_n = s_{n-1} = l$) and a scalar, the state evolution equation simplifies.
We identify the state transition matrix $A_n$ and the process noise $u_n$:
\begin{equation*}
  s_n = 1 \cdot s_{n-1} + 0
\end{equation*}
Thus, identifying terms with the general form $s_n = A_n s_{n-1} + G_n f_n + u_n$:
\begin{itemize}
  \item State variable: $s_n = l$ (scalar).
  \item State transition matrix: $A_n = 1$.
  \item Control input: $G_n = 0$ (no external forcing).
  \item Process noise: $u_n = 0$, which implies the process noise covariance is $Q_n = 0$.
\end{itemize}
For the measurement equation, we relate the pressure observation $x_n$ to the fluid level using the hydrostatic constant $c_h$:
\begin{equation*}
  x_n = c_h \cdot s_n + v_n
\end{equation*}
Comparing this to $x_n = H_n s_n + w_n$:
\begin{itemize}
  \item Observation matrix: $H_n = c_h$.
  \item Measurement noise: $w_n = v_n$.
  \item Measurement noise covariance: $R_n = \sigma_v^2$ (given variance of $v_k$).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
\question{Question:Derive the Kalman filter equations for this problem, in order to obtain the LMMSE estimate of the state given the observations $x_0$,\ldots,$x_n$ ( i.e., $\hat s_{n|n}$).}
\vspace{0.5cm}

\subsubsection*{Derivation of the Kalman Filter Equations}

Since the system state is a scalar constant ($s_n = s_{n-1}$), we identified that $A_n = 1$, $G_n = 0$, $Q_n = 0$. The measurement follows $x_n = c_h s_n + v_n$, implying $H_n = c_h$ and $R_n = \sigma_v^2$. The Kalman filter recursive steps for $n \ge 0$ are derived as follows:

\begin{enumerate}
  \item \textbf{Prediction ($\hat{s}_{n|n-1}$):}
        Using the state evolution equation with $f_n=0$:
        \begin{equation*}
          \hat{s}_{n|n-1} = A_n \hat{s}_{n-1|n-1} = 1 \cdot \hat{s}_{n-1|n-1} = \hat{s}_{n-1|n-1}
        \end{equation*}

  \item \textbf{Prediction Error Covariance ($\Sigma_{n|n-1}$):}
        Using the general prediction covariance formula with $Q_n=0$:
        \begin{equation*}
          \Sigma_{n|n-1} = A_n \Sigma_{n-1|n-1} A_n^T + Q_n = 1 \cdot \Sigma_{n-1|n-1} \cdot 1 + 0 = \Sigma_{n-1|n-1}
        \end{equation*}

  \item \textbf{Kalman Gain ($K_n$):}
        Since $H_n$ and $R_n$ are scalars ($c_h$ and $\sigma_v^2$), the matrix inversion becomes a scalar division:
        \begin{equation*}
          K_n = \Sigma_{n|n-1} H_n^T (H_n \Sigma_{n|n-1} H_n^T + R_n)^{-1}
        \end{equation*}
        Substituting the known values:
        \begin{equation*}
          K_n = \frac{\Sigma_{n|n-1} \cdot c_h}{c_h^2 \Sigma_{n|n-1} + \sigma_v^2}
        \end{equation*}

  \item \textbf{Correction ($\hat{s}_{n|n}$):}
        Updating the a priori estimate with the new measurement $x_n$:
        \begin{equation*}
          \hat{s}_{n|n} = \hat{s}_{n|n-1} + K_n (x_n - H_n \hat{s}_{n|n-1})
        \end{equation*}
        Substituting the prediction from step 1:
        \begin{equation*}
          \hat{s}_{n|n} = \hat{s}_{n-1|n-1} + K_n (x_n - c_h \hat{s}_{n-1|n-1})
        \end{equation*}

  \item \textbf{Estimation Error Covariance ($\Sigma_{n|n}$):}
        Updating the error covariance. Note that the identity matrix $\mathbf{I}$ becomes the scalar $1$:
        \begin{equation*}
          \Sigma_{n|n} = (I - K_n H_n) \Sigma_{n|n-1}
        \end{equation*}
        Substituting scalars and the result from step 2:
        \begin{equation*}
          \Sigma_{n|n} = (1 - K_n c_h) \Sigma_{n-1|n-1}
        \end{equation*}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
\question{Question: How should we choose the initial estimate $\hat s_{-1|-1}$ and estimation error covariance $\Sigma_{-1|-1}$?}
\vspace{0.5cm}

Before collecting any measurements, we encode our a priori knowledge about the fluid level:

\begin{enumerate}
  \item \textbf{$\hat{s}_{-1|-1}$:}
        The best guess a priori for the fluid level is its prior mean:
        \begin{equation*}
          \hat{s}_{-1|-1} = \mu_\ell
        \end{equation*}

  \item \textbf{$\Sigma_{-1|-1}$:}
        Our uncertainty about this guess is quantified by the prior variance:
        \begin{equation*}
          \Sigma_{-1|-1} = \sigma_\ell^2
        \end{equation*}
\end{enumerate}

\textbf{Connection to Task 2:} These initial conditions ensure that $\hat{\ell}_0$ (obtained from the Kalman filter with $n=0$) matches the LMMSE formula with one observation, and similarly for $\Sigma_{0|0}$. This validates our choices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
\question{Question: Write a Matlab script (You may use the file {\tt kalman\_dc\_template.m} as starting point) to simulate this dynamical system and the Kalman filter. Assume that the sensor measures pressure in mbar, fluid level is in cm, and that the fluid is benzene (density $874$ kg/m$^3$ at 25$^\circ$C).
  Our initial guess of the fluid level is $\mu_\ell=250$ cm, and we assign a standard deviation $\sigma_\ell = 11$ cm to reflect our uncertainty about this guess. Measurement errors are uncorrelated, zero-mean Gaussian with standard deviation $\sigma_v = 25$ mbar, and the true fluid level is $340$ cm. Plot the time evolution of the state, the measurements, and the estimate, up to $n=200$, and for two different executions. Also, plot the time evolution of the standard deviation of the fluid level estimation error, in cm. Comment on your results.
}

We implemented the Kalman filter~\ref{app:kalman_dc} using the provided MATLAB script to estimate the level of a constant fluid in a tank. The system parameters are defined as follows: Benzene density $\rho \approx 0.874$ g/cm$^3$, hydrostatic constant $c_p = 0.98\rho$, true level $s=340$ cm, and initial guess $\mu_s = 250$ cm with uncertainty $\sigma_s = 11$ cm.

\subsubsection*{Simulation Results}

The simulation was executed twice to observe the effect of random noise realizations. The results are shown in Figure \ref{fig:exec1} and Figure \ref{fig:exec2}.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{img/task3_4_1.png}
  \caption{Execution 1: Time evolution of the state, measurements, and estimation error standard deviation.}
  \label{fig:exec1}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{img/task3_4_2.png}
  \caption{Execution 2: Comparison with a different noise realization.}
  \label{fig:exec2}
\end{figure}

\subsubsection*{Comments on Results}

Based on the generated plots, we can draw the following conclusions:

\begin{itemize}
  \item \textbf{Convergence of the Estimate:} In the top subplot of both figures, the estimate (red solid line) starts at the initial guess of $250$ cm. It rapidly corrects this initial error, converging towards the true fluid level (blue dashed line at $340$ cm) within the first 20 to 30 iterations. The filter effectively smooths the noisy data.

  \item \textbf{Measurements in Different Units:} The observations (cyan stars) are pressure in mbar, while level and estimate are in cm. Since $c_h \approx 0.856$ mbar/cm, the true level 340 cm gives pressure $340 \times 0.856 \approx 291$ mbar, explaining the apparent shift. The filter handles this via $H_n = c_h$.

  \item \textbf{Error Covariance Evolution:} The bottom subplot shows the standard deviation of the estimation error ($\sqrt{P_{n|n}}$). It decreases monotonically from the initial uncertainty ($\sigma_l = 11$ cm) towards zero. This asymptotic behavior is expected because the system is static with no process noise ($Q=0$). The filter accumulates information indefinitely, theoretically achieving perfect precision as $n \to \infty$.

  \item \textbf{Independence from Realizations:} Comparing Figure \ref{fig:exec1} and Figure \ref{fig:exec2}, while the specific noise samples (cyan stars) and the exact path of the estimate (red line) differ due to randomness, the error standard deviation curve (bottom plot) is \textbf{identical} in both cases. This confirms that the covariance evolution in the Kalman filter depends only on the system matrices ($A, H, R, Q$) and is deterministic, independent of the actual measurement values.
\end{itemize}
\vspace{0.5cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
\question{Question: Using mathematical induction, prove that $\Sigma_{n|n} = \frac{\sigma_\ell^2}{1+(n+1)\alpha c_h^2}$ for all $n\geq 0$. Does this expression look familiar? Evaluate it for $n=200$ with the parameters of the previous point, and check the result of your simulation.  What is the asymptotic value of $\Sigma_{n|n}$?}

\vspace{0.5cm}
\begin{itemize}
  \item For n = 0, $\Sigma_{0|-1} = \Sigma_{-1|-1} = \sigma_\ell^2$, $Q=0$ and $A=1$:
        $$K_0 = \frac{\Sigma_{0|-1} c_h}{c_h^2 \Sigma_{0|-1} + \sigma_v^2} = \frac{\sigma_\ell^2 c_h}{c_h^2 \sigma_\ell^2 + \sigma_v^2}$$
        $$\Sigma_{0|0} = (1 - K_0 c_h) \Sigma_{0|-1}$$
        Substuying $K_0$:
        $$\Sigma_{0|0} = \left( 1 - \frac{\sigma_\ell^2 c_h^2}{c_h^2 \sigma_\ell^2 + \sigma_v^2} \right) \sigma_\ell^2 = \left( \frac{c_h^2 \sigma_\ell^2 + \sigma_v^2 - \sigma_\ell^2 c_h^2}{c_h^2 \sigma_\ell^2 + \sigma_v^2} \right) \sigma_\ell^2$$
        $$\Sigma_{0|0} = \frac{\sigma_v^2 \sigma_\ell^2}{c_h^2 \sigma_\ell^2 + \sigma_v^2} = \frac{\sigma_\ell^2}{c_h^2 \frac{\sigma_\ell^2}{\sigma_v^2} + 1} = \frac{\sigma_\ell^2}{1 + \alpha c_h^2}$$
        So, cheking the formula for n=0:
        $$\frac{\sigma_\ell^2}{1+(0+1)\alpha c_h^2} = \frac{\sigma_\ell^2}{1+\alpha c_h^2}$$
  \item For n = k:
        We know that $\Sigma_{k|k-1} = \Sigma_{k-1|k-1}$, so
        $$\frac{1}{\Sigma_{k|k}} = \frac{1}{\Sigma_{k|k-1}} + \frac{H^2}{R} = \frac{1}{\Sigma_{k-1|k-1}} + \frac{c_h^2}{\sigma_v^2}$$
        $$\frac{1}{\Sigma_{k|k}} = \frac{1 + k\alpha c_h^2}{\sigma_\ell^2} + \frac{c_h^2}{\sigma_v^2}$$
        $\alpha = \frac{\sigma_\ell^2}{\sigma_v^2}$, so $\frac{1}{\sigma_v^2} = \frac{\alpha}{\sigma_\ell^2}$
        $$\frac{1}{\Sigma_{k|k}} = \frac{1 + k\alpha c_h^2}{\sigma_\ell^2} + \frac{c_h^2 \alpha}{\sigma_\ell^2}$$
        $$\frac{1}{\Sigma_{k|k}} = \frac{1 + k\alpha c_h^2 + \alpha c_h^2}{\sigma_\ell^2} = \frac{1 + (k+1)\alpha c_h^2}{\sigma_\ell^2}$$
        $$\Sigma_{k|k} = \frac{\sigma_\ell^2}{1 + (k+1)\alpha c_h^2}$$
\end{itemize}

\subsubsection{Evaluate it for $n = 200$:}
\begin{itemize}
  \item $\sigma_\ell = 11 cm \implies \sigma_\ell^2 = 121 $
  \item $\sigma_v = 25 mbar \implies \sigma_v^2 = 625$
  \item $\alpha = \frac{121}{625} = 0.1936$
  \item Density $\rho = 0.874g/cm^3$
  \item $c_h = 0.98 \times \rho = 0.98 \times 0.874 = 0.85652 mbar/cm$
  \item $c_h^2 \approx 0.7336$
\end{itemize}

$$\Sigma_{200|200} = \frac{121}{1 + (200+1) \cdot 0.1936 \cdot 0.7336}$$
$$\Sigma_{200|200} = \frac{121}{1 + 201 \cdot 0.1420}$$
$$\Sigma_{200|200} = \frac{121}{1 + 28.547} = \frac{121}{29.547} \approx 4.095 \text{ cm}^2$$

$$\sqrt{\Sigma_{200|200}} = \sqrt{4.095} \approx \mathbf{2.02 \text{ cm}}$$
In the plot of the Figure \ref{fig:kalman_sigma} we can see how the curve drop to approximately 2 at $n = 200$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
\question{Question: Show that
  \begin{equation*}\label{eq:est_2}
    \hat s_{n|n} = \frac{1+n\alpha c_h^2}{1+n\alpha c_h^2 + \alpha c_h^2} \hat s_{n-1|n-1} + \frac{\alpha c_h}{1+ n\alpha c_h^2 + \alpha c_h^2}x_n.
  \end{equation*}
  Compare this with your answer to the first point of Task 2 and give your conclusions.}

\vspace{0.5cm}

\subsubsection*{Derivation}

From the Kalman filter correction step, we have:
\begin{equation*}
  \hat{s}_{n|n} = \hat{s}_{n-1|n-1} + K_n(x_n - c_h \hat{s}_{n-1|n-1})
\end{equation*}

Rearranging:
\begin{equation*}
  \hat{s}_{n|n} = (1 - K_n c_h)\hat{s}_{n-1|n-1} + K_n x_n
\end{equation*}

From the previous question, we proved that:
\begin{equation*}
  \Sigma_{n|n} = \frac{\sigma_\ell^2}{1+(n+1)\alpha c_h^2}
\end{equation*}

We also know that $\Sigma_{n|n-1} = \Sigma_{n-1|n-1} = \frac{\sigma_\ell^2}{1+n\alpha c_h^2}$ (since $Q_n=0$).

The Kalman gain is:
\begin{equation*}
  K_n = \frac{\Sigma_{n|n-1} c_h}{c_h^2 \Sigma_{n|n-1} + \sigma_v^2}
\end{equation*}

Substituting $\Sigma_{n|n-1}$:
\begin{align*}
  K_n & = \frac{\frac{\sigma_\ell^2}{1+n\alpha c_h^2} c_h}{c_h^2 \frac{\sigma_\ell^2}{1+n\alpha c_h^2} + \sigma_v^2}                                  \\
      & = \frac{\sigma_\ell^2 c_h}{c_h^2 \sigma_\ell^2 + \sigma_v^2(1+n\alpha c_h^2)}                                                                 \\
      & = \frac{\sigma_\ell^2 c_h}{\sigma_v^2(1+n\alpha c_h^2 + \alpha c_h^2)} \quad \text{(using } \alpha = \frac{\sigma_\ell^2}{\sigma_v^2}\text{)} \\
      & = \frac{\alpha c_h}{1+(n+1)\alpha c_h^2}
\end{align*}

Now, compute $1 - K_n c_h$:
\begin{align*}
  1 - K_n c_h & = 1 - \frac{\alpha c_h^2}{1+(n+1)\alpha c_h^2}                   \\
              & = \frac{1+(n+1)\alpha c_h^2 - \alpha c_h^2}{1+(n+1)\alpha c_h^2} \\
              & = \frac{1+n\alpha c_h^2}{1+(n+1)\alpha c_h^2}
\end{align*}

Substituting back into the correction equation:
\begin{equation*}
  \boxed{\hat{s}_{n|n} = \frac{1+n\alpha c_h^2}{1+(n+1)\alpha c_h^2} \hat{s}_{n-1|n-1} + \frac{\alpha c_h}{1+(n+1)\alpha c_h^2} x_n}
\end{equation*}

Note that $1+(n+1)\alpha c_h^2 = 1+n\alpha c_h^2 + \alpha c_h^2$, so this is exactly the desired form.

\subsubsection*{Comparison with Task 2}

In Task 2, Question 1, we derived the recursive LMMSE estimator:
\begin{equation*}
  \hat{\ell}_n = \frac{1+n\alpha c_h^2}{1+(n+1)\alpha c_h^2} \hat{\ell}_{n-1} + \frac{\alpha c_h}{1+(n+1)\alpha c_h^2} x_n
\end{equation*}

\textbf{Conclusion:} The Kalman filter estimate $\hat{s}_{n|n}$ is \textbf{identical} to the recursive LMMSE estimator $\hat{\ell}_n$ from Task 2. This confirms a fundamental result:

\begin{center}
  \fbox{\parbox{0.9\textwidth}{%
      \textbf{The Kalman filter is the optimal recursive implementation of the LMMSE estimator for linear Gaussian systems.}
    }}
\end{center}

Both approaches yield the same estimate, but the Kalman filter framework provides:
\begin{itemize}
  \item A systematic way to incorporate process dynamics (state evolution)
  \item Error covariance tracking ($\Sigma_{n|n}$)
  \item Generalization to multivariate and time-varying systems
  \item A recursive structure with constant computational complexity $O(1)$
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
\question{Question: Repeat your simulations after changing $\sigma_\ell$ to 50 cm. Repeat again for $\sigma_\ell = 5$ cm. Comment on your results.}
\subsubsection*{Analysis of Sensitivity to Initial Uncertainty}

\section*{Analysis of Sensitivity to Initial Uncertainty ($\sigma_\ell$)}

The simulations were repeated modifying the standard deviation of the initial guess, $\sigma_\ell$, to analyze how the filter's confidence in the initial state affects the estimation process.

\subsubsection*{Case 1: High Initial Uncertainty ($\sigma_\ell = 50$ cm)}

In this scenario, we set $\sigma_\ell = 50$ cm, which implies a very high initial variance ($P_{0|-1} = 2500$ cm$^2$). This indicates to the filter that the initial guess $\mu_\ell = 250$ cm is unreliable.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{img/50_1.png}
    \caption{Execution 1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{img/50_2.png}
    \caption{Execution 2}
  \end{subfigure}
  \caption{Simulation results for $\sigma_\ell = 50$ cm. The estimate (red line) jumps almost immediately to the true level.}
  \label{fig:sigma50}
\end{figure}

\textbf{Comments on Results:}
As observed in Figure \ref{fig:sigma50}, the convergence is extremely fast. In the very first iterations ($n=0, 1$), the estimate jumps from $250$ cm to the vicinity of the true value ($340$ cm).
\begin{itemize}
  \item Because the initial uncertainty is high relative to the measurement noise ($\sigma_\ell^2 \gg \sigma_v^2$), the Kalman Gain is close to 1.
  \item The filter places almost all weight on the incoming measurements and ignores the initial guess.
  \item While this corrects the initial bias quickly, the estimate is more susceptible to measurement noise in the initial steps.
\end{itemize}

\subsubsection*{Case 2: Low Initial Uncertainty ($\sigma_\ell = 5$ cm)}

In this scenario, we set $\sigma_\ell = 5$ cm. This implies a high confidence in the initial guess ($P_{0|-1} = 25$ cm$^2$). The filter assumes the initial value of $250$ cm is likely correct.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{img/5_1.png}
    \caption{Execution 1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{img/5_2.png}
    \caption{Execution 2}
  \end{subfigure}
  \caption{Simulation results for $\sigma_\ell = 5$ cm. The estimate (red line) converges very slowly towards the true level.}
  \label{fig:sigma5}
\end{figure}

\textbf{Comments on Results:}
Figure \ref{fig:sigma5} shows a much slower convergence (sluggish response). The estimate starts at $250$ cm and takes a significant number of iterations (over 50 samples) to get close to the true level of $340$ cm.
\begin{itemize}
  \item Because the initial uncertainty is low ($\sigma_\ell^2 \ll \sigma_v^2$), the Kalman Gain is initially very small.
  \item The filter trusts the initial prediction heavily and treats the incoming measurements (which differ by $\approx 90$ cm) as noise/outliers.
  \item This is a case of \textit{overconfidence}: the filter relies too much on an incorrect prior, leading to a persistent bias that takes time to correct.
\end{itemize}

\subsubsection*{Conclusion}
The comparison demonstrates that $\sigma_\ell$ controls the **speed of convergence**:
\begin{itemize}
  \item A **large $\sigma_\ell$** allows the filter to "forget" the initial guess quickly and trust the data, which is preferred when the initial state is not well known.
  \item A **small $\sigma_\ell$** acts as an "anchor," making the filter resistant to noise but also slow to react to large initial errors.
\end{itemize}