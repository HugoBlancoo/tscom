Consider now the case in which pressure measurements arrive sequentially in time (at this point, the fluid level is assumed to remain constant through time). Therefore, at time $nT$ ($T$ is the sampling interval), a new measurement $x_n$ arrives, and we can refine our LMMSE estimator of the fluid level incorporating the new information available. Let us change the notation slightly to make the dependence on the time index more clear. Thus, now we denote the LMMSE estimate of the fluid level  based on these measurements as $\hat \ell_{n-1}$. Then, from \eqref{eq:lmmse_ave}, we know that
\begin{equation}\label{eq:brute}
  \hat \ell_{n-1} = \frac{1}{1+n\alpha c_h^2} \mu_\ell + \frac{\alpha c_h}{1+n\alpha c_h^2} (x_0 + x_1 + \ldots + x_{n-1}).
\end{equation}
When the new measurement $x_n$ arrives, we could use a similar expression to compute the LMMSE estimate $\hat \ell_n$. However, we may exploit the availability of $\hat \ell_{n-1}$ to save computations.

\vspace{0.25cm}
\noindent The detailed derivations for all questions in this section are available in Appendix~\ref{app:handwritten}.

\vspace{0.5cm}
\question{Question: Obtain $\hat \ell_n$ as a linear combination of $\hat \ell_{n-1}$ and the new measurement $x_n$.}
\vspace{0.5cm}

Recall that the LMMSE estimator at step $n-1$ and step $n$ are:
\begin{align*}
  \hat\ell_{n-1} & = \frac{1}{1+n\alpha c_h^2}\mu_\ell + \frac{\alpha c_h}{1+n\alpha c_h^2} S_{n-1}, \\
  \hat\ell_n     & = \frac{1}{1+(n+1)\alpha c_h^2}\mu_\ell + \frac{\alpha c_h}{1+(n+1)\alpha c_h^2} S_n,
\end{align*}

where $S_{n-1} = \sum_{k=0}^{n-1} x_k$ and $S_n = S_{n-1} + x_n$.

From the expression for $\hat\ell_{n-1}$, we can isolate $S_{n-1}$:
\begin{equation*}
  S_{n-1} = \frac{1+n\alpha c_h^2}{\alpha c_h}\hat\ell_{n-1} - \frac{1}{\alpha c_h}\mu_\ell
\end{equation*}

Substituting into the expression for $\hat\ell_n$:
\begin{align*}
  \hat\ell_n 
  &= \frac{1}{1+(n+1)\alpha c_h^2}\mu_\ell + \frac{\alpha c_h}{1+(n+1)\alpha c_h^2}\left(\frac{1+n\alpha c_h^2}{\alpha c_h}\hat\ell_{n-1} - \frac{1}{\alpha c_h}\mu_\ell + x_n\right) \\
  &= \frac{1}{1+(n+1)\alpha c_h^2}\mu_\ell + \frac{1+n\alpha c_h^2}{1+(n+1)\alpha c_h^2}\hat\ell_{n-1} \\
  &\quad - \frac{1}{1+(n+1)\alpha c_h^2}\mu_\ell + \frac{\alpha c_h}{1+(n+1)\alpha c_h^2}x_n
\end{align*}

The $\mu_\ell$ terms cancel, yielding:
\begin{equation*}
  \boxed{\hat\ell_n = \frac{1+n\alpha c_h^2}{1+(n+1)\alpha c_h^2}\hat\ell_{n-1} + \frac{\alpha c_h}{1+(n+1)\alpha c_h^2}x_n}
\end{equation*}

This is the recursive update formula: each new estimate is a weighted combination of the previous estimate and the new measurement.

The detailed derivation can be found in Appendix~\ref{app:handwritten}, Figure~\ref{fig:task2_q1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
\question{Question: Show that, for appropriate initial values $\beta_{-1}$ and $\hat\ell_{-1}$ that you should specify, the LMMSE estimate can be computed recursively as
  \[  \beta_n = \beta_{n-1} + \alpha c_h^2, \qquad \hat \ell_n = \frac{1}{\beta_n} (\beta_{n-1}\hat \ell_{n-1} + \alpha c_h x_n), \qquad n\geq 0. \]
  Using this recursion, how many additions, multiplications and divisions do we need in order to update the estimate with each new observation? Does this load depend on the time index $n$?}
\vspace{0.5cm}

\subsubsection*{Recursive form with $\beta_n$}

Define:
\begin{equation*}
  \beta_n = 1 + (n+1)\alpha c_h^2, \qquad \beta_{n-1} = 1 + n\alpha c_h^2
\end{equation*}

Then clearly:
\begin{equation*}
  \boxed{\beta_n = \beta_{n-1} + \alpha c_h^2}
\end{equation*}

Substituting $\beta_n$ and $\beta_{n-1}$ into the recursive formula from the previous question:
\begin{equation*}
  \hat\ell_n = \frac{\beta_{n-1}}{\beta_n}\hat\ell_{n-1} + \frac{\alpha c_h}{\beta_n}x_n
\end{equation*}

Multiplying both sides by $\beta_n$:
\begin{equation*}
  \beta_n \hat\ell_n = \beta_{n-1}\hat\ell_{n-1} + \alpha c_h x_n
\end{equation*}

Therefore:
\begin{equation*}
  \boxed{\hat\ell_n = \frac{1}{\beta_n}(\beta_{n-1}\hat\ell_{n-1} + \alpha c_h x_n)}
\end{equation*}

\subsubsection*{Initial values}

For $n=0$ (first observation), with no prior measurements, the best estimate is the prior mean:
\begin{equation*}
  \hat\ell_{-1} = \mu_\ell
\end{equation*}

And from the definition:
\begin{equation*}
  \beta_{-1} = 1 + 0 \cdot \alpha c_h^2 = 1
\end{equation*}

\textbf{Verification:} Using the recursion with $n=0$:
\begin{equation*}
  \hat\ell_0 = \frac{1}{\beta_0}(\beta_{-1}\hat\ell_{-1} + \alpha c_h x_0)
  = \frac{1}{1+\alpha c_h^2}(\mu_\ell + \alpha c_h x_0),
\end{equation*}
which matches the LMMSE formula with one observation.

Therefore:
\begin{equation*}
  \boxed{\beta_{-1} = 1, \qquad \hat\ell_{-1} = \mu_\ell}
\end{equation*}

\subsubsection*{Computational complexity}

To update from $\hat\ell_{n-1}$ to $\hat\ell_n$:

\begin{enumerate}
  \item Update $\beta_n = \beta_{n-1} + \alpha c_h^2$: \textbf{1 addition}
  \item Compute $\beta_{n-1}\hat\ell_{n-1}$: \textbf{1 multiplication}
  \item Compute $\alpha c_h x_n$: \textbf{1 multiplication}
  \item Add: $\beta_{n-1}\hat\ell_{n-1} + \alpha c_h x_n$: \textbf{1 addition}
  \item Divide by $\beta_n$: \textbf{1 division}
\end{enumerate}

\textbf{Total per iteration:}
\begin{itemize}
  \item 2 additions
  \item 2 multiplications
  \item 1 division
\end{itemize}

\textbf{Does it depend on $n$?} \textbf{No.} The computational load is constant ($O(1)$) and only requires storing two scalars: $\beta_{n-1}$ and $\hat\ell_{n-1}$.

\subsubsection*{Comparison with direct computation}

Computing the LMMSE naively using the direct formula:
\begin{equation*}
  \hat\ell_n = \frac{1}{1+(n+1)\alpha c_h^2}\mu_\ell + \frac{\alpha c_h}{1+(n+1)\alpha c_h^2}\sum_{k=0}^{n} x_k
\end{equation*}
requires:
\begin{itemize}
  \item \textbf{Memory:} Store all $n+1$ observations $x_0, x_1, \ldots, x_n$ → $O(n)$
  \item \textbf{Additions:} Computing the sum requires $n$ additions → $O(n)$
  \item \textbf{Scalability:} Both memory and computation grow linearly with $n$
\end{itemize}

In contrast, the recursive approach requires \textbf{constant memory and computation}, making it vastly more efficient for long observation sequences.

The detailed derivation can be found in Appendix~\ref{app:handwritten}, Figure~\ref{fig:task2_q2}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
\question{Note that if one simply stored all samples in memory and then computed  the LMMSE estimate naively using \eqref{eq:brute} without using recursivity, then the memory size and the number of additions would grow with $n$.}
\vspace{0.5cm}

Specifically, using the direct formula:
\begin{equation*}
  \hat\ell_{n-1} = \frac{1}{1+n\alpha c_h^2}\mu_\ell + \frac{\alpha c_h}{1+n\alpha c_h^2}(x_0 + x_1 + \cdots + x_{n-1}),
\end{equation*}
requires:
\begin{itemize}
  \item \textbf{Memory:} Store all $n$ observations $x_0, x_1, \ldots, x_{n-1}$.
  \item \textbf{Additions:} Computing the sum $x_0 + x_1 + \cdots + x_{n-1}$ requires $n-1$ additions.
  \item \textbf{Scalability:} Both memory and computational cost grow linearly with $n$.
\end{itemize}

In contrast, the recursive approach requires only constant memory ($O(1)$) and constant computational cost per update, making it much more efficient for processing long sequences of observations.
